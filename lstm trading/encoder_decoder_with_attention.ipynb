{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import *\n",
    "from torchvision import datasets\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "from torchmetrics import *\n",
    "\n",
    "from torch.utils.data import *\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time step to predict ahead, creates input sequences\n",
    "# gonna have to figure out test\n",
    "def create_lstm_data(data, time_step=1, future = 0,test_percent = .1):\n",
    "    x_vec, y_vec = [], []\n",
    "    # formats data so y = t and x = t-1, ... , t-time_steps\n",
    "    for i in range(len(data) - time_step):\n",
    "        split = i + time_step\n",
    "        # checks if data is of same length for concatentation\n",
    "        length = data[split : split + future, 0].shape[0]\n",
    "        if(length == future):\n",
    "            x_vec.append(data[i : split, 0].unsqueeze(0))\n",
    "            y_vec.append(data[split : split + future, 0].unsqueeze(0))\n",
    "    # calculate number of elements to allocate to test\n",
    "    #dataset_length = len(x_vec)\n",
    "    #num_of_train = dataset_length - (int)(test_percent * dataset_length)\n",
    "    # concats x into matrix and y into vector, needs unsqueez to add single dimension for LSTM\n",
    "    return torch.cat(x_vec,0).unsqueeze(-1), torch.cat(y_vec,0).unsqueeze(-1)#, torch.cat(x_vec[num_of_train:],0).unsqueeze(-1), torch.cat(y_vec[num_of_train:],0).unsqueeze(-1)\n",
    "\n",
    "# create a dataset out of timeseries data, must be formatted first, tensor\n",
    "# correct timeseries formatation\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.X[i], self.y[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possibly add attention\n",
    "#possibly change how layers of encoder to decoder are handled (possibly make different sizes)\n",
    "#add batch norm\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout = 0.5):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout,\n",
    "                           batch_first=True, bidirectional = True)\n",
    "        \n",
    "        #to choose part of birdectinals were important\n",
    "        self.fc_hidden = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        #chooses which way of directionsal is most important\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1],hidden[1:2]), dim = 2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1],cell[1:2]), dim = 2))\n",
    "        \n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout = 0.5):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout,\n",
    "                           )\n",
    "        \n",
    "        #mini attention layer\n",
    "        self.energy = nn.Linear(hidden_dim*3,1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    #predicts a single time step, with either y pred as input or y\n",
    "    def forward(self, y, encoder_output, prev_hidden, prev_cell):\n",
    "        \n",
    "        print(f\"Decoder-input-shape: {y.shape}\")\n",
    "        print(f\"Encoder-output-shape: {encoder_output.shape}\")\n",
    "        print(f\"Prev-hidden-shape: {prev_hidden.shape}\")\n",
    "        \n",
    "        #going to have to list dimensions of every vector to see what works\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        sequence_length = prev_hidden.shape[0]\n",
    "\n",
    "        hidden_reshape = hidden.repeat(sequence_length,1,1)\n",
    "        print(f\"hidden-repeat-shape: {prev_hidden.shape}\")\n",
    "        energy = self.relu(self.energy(torch.cat((hidden_reshape,encoder_output),dim = 2)))\n",
    "        print(f\"energy-shape: {energy.shape}\")\n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        #change dim of it to element wise multiply it with encoder states\n",
    "        attention = attention.permute(1,2,0)\n",
    "        encoder_states = encoder_states.permute(1,2,0)\n",
    "        \n",
    "        #change dim to fit with input(y)\n",
    "        context_vector = torch.bmm(attention,encoder_states).permute(1,0,2)\n",
    "        rnn_input = torch.cat((context_vector, y))\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (prev_hidden, prev_cell))\n",
    "        \n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        prediction = prediction.squeeze(0)\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class EncoderDecoderWrapper(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_size = 1, teacher_forcing=0.3, device = 'cpu'):\n",
    "        super(EncoderDecoderWrapper,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.device = device\n",
    "        self.output_size = output_size\n",
    "        # might change if layers arent the same proves different results\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        )\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        )\n",
    "    def forward(self, source, target=None):\n",
    "        \n",
    "        # num of elements in each batch\n",
    "        batch_size = source.shape[0]\n",
    "        #should be same size as output\n",
    "        target_len = target.shape[1]\n",
    "        \n",
    "        assert(target_len == self.output_size)\n",
    "        \n",
    "        encoder_output, prev_hidden, prev_cell = self.encoder(source)\n",
    "\n",
    "        prev_target = source[:,-1]\n",
    "            \n",
    "        outputs = torch.zeros(batch_size,self.output_size).to(self.device)\n",
    "        \n",
    "        for t in range(self.output_size):\n",
    "            \n",
    "            prediction, prev_hidden, prev_cell = self.decoder(prev_target, prev_hidden, prev_cell)\n",
    "\n",
    "            outputs[:,t] = prediction.squeeze(1)\n",
    "            \n",
    "            #chance of using actual vs chance of using predicted in training\n",
    "            prev_target = target[:,t] if torch.rand(1) < self.teacher_forcing or target != None else prediction\n",
    "        \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
