{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal Gated Unit in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative = False):\n",
    "    \n",
    "    z = 1/ (1 + np.exp(-x)) \n",
    "    \n",
    "    if derivative:\n",
    "        return z * (1 - z)\n",
    "    else: \n",
    "        return z\n",
    "\n",
    "def tanh(x, derivative = False):\n",
    "    \n",
    "    z = np.tanh(x)\n",
    "    \n",
    "    if derivative:\n",
    "        return 1 - np.square(z)\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mru(hidden_size, output_size, batch_size):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Forget gate weights and biase\n",
    "    U_f = np.random.randn(hidden_size,hidden_size) * 0.1 - 0.05\n",
    "    W_f = np.random.randn(hidden_size,batch_size) * 0.1 - 0.05\n",
    "    b_f = np.zeros((hidden_size,1))\n",
    "    \n",
    "    # Hidden_hat weights\n",
    "    U_h = np.random.randn(hidden_size,hidden_size) * 0.1 - 0.05\n",
    "    W_h = np.random.randn(hidden_size,batch_size) * 0.1 - 0.05\n",
    "    b_h = np.zeros((hidden_size,1))\n",
    "    \n",
    "    # Output weights\n",
    "    W_y = np.random.rand(output_size, hidden_size) * 0.1 - 0.05\n",
    "    b_y = np.random.rand(output_size, 1)\n",
    "    \n",
    "    # Previous Hidden\n",
    "    prev_hidden = np.zeros((hidden_size,batch_size))\n",
    "    \n",
    "    params = [W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y]\n",
    "    \n",
    "    return prev_hidden, params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Forward Pass Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mru_cell_forward(x, prev_hidden, params):\n",
    "    \n",
    "    # unpack parameters\n",
    "    W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y = params\n",
    "    \n",
    "    # Calculate forget gate, expand dim to preserve dim size\n",
    "    f = sigmoid(np.expand_dims(np.dot(W_f, x),1) + np.dot(U_f, prev_hidden) + b_f)\n",
    "\n",
    "    # Calculate hidden hat, expand dim to preserve dim size\n",
    "    h_hat = tanh(np.expand_dims(np.dot(W_h,x),1) + np.dot(U_h, np.multiply(f, prev_hidden)) + b_h)\n",
    "    \n",
    "    # Calculate hidden\n",
    "    hidden = np.multiply((1 - f),prev_hidden) + np.multiply(f,h_hat)\n",
    "    \n",
    "    # Calculate prediction\n",
    "    y_pred = np.dot(W_y, hidden) + b_y\n",
    "    \n",
    "    \n",
    "    cache = (x, prev_hidden, f, h_hat, hidden, y_pred, params)\n",
    "    \n",
    "    return hidden, y_pred, cache\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mru_forward(x, h0, params):\n",
    "    \n",
    "    caches = []\n",
    "    \n",
    "    # Shape of input: num_batches x num_timesteps\n",
    "    n, T_x = x.shape\n",
    "    h_size, n = h0.shape\n",
    "    # Initialize hidden\n",
    "    h_t = h0 \n",
    "    \n",
    "    # Initialize h_t and yt_pred storage\n",
    "    h = np.zeros((h_size,n,T_x))\n",
    "    \n",
    "    # -- batch size x timesteps\n",
    "    y_pred = np.zeros((n,T_x))\n",
    "    # Iterates through all timesteps\n",
    "    for t in range(T_x):\n",
    "        print(f'Time Step: {t}')\n",
    "        # get x's at timestep \n",
    "        x_t = x[:, t]\n",
    "        print()\n",
    "        # Compute forward propagation: new hidden state, y_pred, and cache\n",
    "        h_t, yt_pred, cache = mru_cell_forward(x_t, h_t, params)\n",
    "        \n",
    "        # Save predicted y\n",
    "        y_pred[:,t] = yt_pred.reshape(-1)\n",
    "        \n",
    "        # Save next hidden state\n",
    "        h[:,:,t] = h_t\n",
    "        \n",
    "        # Save cache for backpropagation\n",
    "        caches.append(cache)\n",
    "    \n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return h, y_pred, cache\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Backward Pass Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mru_cell_backward(dh_next, dy, cache):\n",
    "\n",
    "    # Retrieving values from cache\n",
    "    (x, h_prev, f, h_hat, hidden, y_pred, params) = cache\n",
    "    W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y = params\n",
    "    \n",
    "    # Compute derivatives for y parameters\n",
    "    dW_y = np.dot(dy, np.transpose(h_prev))\n",
    "    db_y = dy\n",
    "    # Intermediate derivatives\n",
    "    dh = dh_next\n",
    "    dh_hat = np.multiply(dh,f)\n",
    "    dh_hat_1 =  dh_hat * tanh(h_hat, derivative=True)\n",
    "    \n",
    "    # Compute derivatives for hidden parameters\n",
    "    dW_h = np.dot(dh_hat_1, np.transpose(x))\n",
    "    dU_h = np.dot(dh_hat_1, np.transpose(np.multiply(f, h_prev)))\n",
    "    db_h = dh_hat_1\n",
    "    \n",
    "    # Intermediate derivatives\n",
    "    dfhp = np.dot(np.transpose(U_f),dh_hat_1)\n",
    "    df = np.multiply(dfhp, h_prev)\n",
    "    df_1 = df * sigmoid(f, derivative=True)\n",
    "    \n",
    "    # Compute derivatives for forget gate parameters\n",
    "    dW_f = np.dot(df_1, np.transpose(x))\n",
    "    dU_f = np.dot(df_1, np.transpose(h_prev))\n",
    "    db_f = df_1\n",
    "    \n",
    "    # all influence of previous later to loss\n",
    "    # compute input and prev hidden derivative\n",
    "    dh_prev = 0\n",
    "    dh_prev += np.multiply(dh,(1-f))\n",
    "    dh_prev += np.dot(np.transpose(U_f), df_1)\n",
    "    dh_prev += np.multiply(dfhp,f)\n",
    "    \n",
    "    grads = (dW_h, dU_h, db_h, dW_f, dU_f, db_f, dW_y, db_y)\n",
    "    \n",
    "    return dh_prev, grads\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dy, lr, params, caches):\n",
    "    \n",
    "    W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y  = params\n",
    "    \n",
    "    (caches, x) = caches\n",
    "    \n",
    "    # Get shape of input\n",
    "    n, T_x = x.shape\n",
    "    \n",
    "    # Initialize gradients with correct sizes\n",
    "    dW_h = np.zeros_like(W_f)\n",
    "    dU_h = np.zeros_like(U_f)\n",
    "    db_h = np.zeros_like(b_h)\n",
    "    dW_f = np.zeros_like(W_h)\n",
    "    dU_f = np.zeros_like(U_h)\n",
    "    db_f = np.zeros_like(b_f)\n",
    "    dW_y = np.zeros_like(W_y)\n",
    "    db_y = np.zeros_like(b_y)\n",
    "    \n",
    "    # Compute original dh_next derivative\n",
    "    dh_next = np.dot(dy,W_y)\n",
    "    \n",
    "    # Compute derivates of derivable parameters  for whole sequence\n",
    "    for i in reversed(range(T_x)):\n",
    "        \n",
    "        dh_next, grads = mru_cell_backward(dh_next, dy, caches[i])\n",
    "        \n",
    "        partial_dW_h, partial_dU_h, partial_db_h, partial_dW_f, partial_dU_f, partial_db_f, partial_dW_y, partial_db_y = grads\n",
    "        \n",
    "        # add each steps gradient to self\n",
    "        dW_h += partial_dW_h\n",
    "        dU_h += partial_dU_h\n",
    "        db_h += partial_db_h\n",
    "        dW_f += partial_dW_f\n",
    "        dU_f += partial_dU_f\n",
    "        db_f += partial_db_f\n",
    "        dW_y += partial_dW_y\n",
    "        db_y += partial_db_y\n",
    "    \n",
    "    #Adjust parameters\n",
    "    W_f += lr * dW_f\n",
    "    U_f += lr * dU_f\n",
    "    W_h += lr * dW_h\n",
    "    U_h += lr * dU_h\n",
    "    W_y += lr * dW_y\n",
    "    b_f += lr * db_f\n",
    "    b_h += lr * db_f\n",
    "    b_y += lr * db_y\n",
    "    \n",
    "    params = [W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(predicted, targets):\n",
    "    \n",
    "    assert(predicted.shape == targets.shape)\n",
    "    n, T_x = predicted.shape\n",
    "    \n",
    "    sequence_loss = np.multiply(np.divide(1,n), np.sum(np.square(np.subtract(targets,predicted)),0))\n",
    "    \n",
    "    dy = np.multiply(np.divide(2,n), np.sum(np.subtract(targets,predicted),0))\n",
    "    \n",
    "    print(f'Loss : {sequence_loss}')\n",
    "    \n",
    "    return dy\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRU_train(x, y, params, prev_hidden, iters, lr):\n",
    "    for i in range(iters):\n",
    "        hiddens, preds, caches = mru_forward(x, prev_hidden, params)\n",
    "        dy = lossFun(preds, y)\n",
    "        params = mru_cell_backward(lr, params, caches)\n",
    "    return params   \n",
    "        \n",
    "def MRU_test(x, params, prev_hidden):\n",
    "    hiddens, preds, cache = mru_forward(x, prev_hidden, params)\n",
    "    return preds[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, output_size, batch_size = 5, 1, 3\n",
    "lr = 0.1\n",
    "prev_hidden, params = init_mru(hidden_size, output_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Step: 0\n",
      "\n",
      "(3,)\n",
      "Time Step: 1\n",
      "\n",
      "(3,)\n",
      "Time Step: 2\n",
      "\n",
      "(3,)\n",
      "Time Step: 3\n",
      "\n",
      "(3,)\n",
      "Time Step: 4\n",
      "\n",
      "(3,)\n",
      "Time Step: 5\n",
      "\n",
      "(3,)\n",
      "Time Step: 6\n",
      "\n",
      "(3,)\n",
      "Time Step: 7\n",
      "\n",
      "(3,)\n",
      "Time Step: 8\n",
      "\n",
      "(3,)\n",
      "Time Step: 9\n",
      "\n",
      "(3,)\n",
      "Loss : [0.6380024  0.65215649 0.6598964  0.66410357 0.66638808 0.66763017\n",
      " 0.66830703 0.66867683 0.66887938 0.6689906 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mMRU_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m, in \u001b[0;36mMRU_train\u001b[1;34m(x, y, params, prev_hidden, iters, lr)\u001b[0m\n\u001b[0;32m      3\u001b[0m     hiddens, preds, caches \u001b[38;5;241m=\u001b[39m mru_forward(x, prev_hidden, params)\n\u001b[0;32m      4\u001b[0m     dy \u001b[38;5;241m=\u001b[39m lossFun(preds, y)\n\u001b[1;32m----> 5\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mmru_cell_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m, in \u001b[0;36mmru_cell_backward\u001b[1;34m(dh_next, dy, cache)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmru_cell_backward\u001b[39m(dh_next, dy, cache):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Retrieving values from cache\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     (x, h_prev, f, h_hat, hidden, y_pred, params) \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m      5\u001b[0m     W_f, U_f, W_h, U_h, W_y, b_f, b_h, b_y \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute derivatives for y parameters\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 5)"
     ]
    }
   ],
   "source": [
    "x = np.ones((batch_size,10))\n",
    "y = np.ones((batch_size,10))\n",
    "MRU_train(x, y, params, prev_hidden, 10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_f, U_f, U_h, W_h, W_y, b_f, b_h, b_y = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78727981, 0.78635477, 0.78583911, 0.78556823, 0.78543314,\n",
       "        0.78536888, 0.78533975, 0.78532727, 0.78532233, 0.78532062],\n",
       "       [0.78727981, 0.78635477, 0.78583911, 0.78556823, 0.78543314,\n",
       "        0.78536888, 0.78533975, 0.78532727, 0.78532233, 0.78532062],\n",
       "       [0.78727981, 0.78635477, 0.78583911, 0.78556823, 0.78543314,\n",
       "        0.78536888, 0.78533975, 0.78532727, 0.78532233, 0.78532062]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41335591, -0.41335591, -0.41335591],\n",
       "        [-0.24835662, -0.24835662, -0.24835662],\n",
       "        [ 0.05868696,  0.05868696,  0.05868696],\n",
       "        [ 0.18363383,  0.18363383,  0.18363383],\n",
       "        [-0.25645071, -0.25645071, -0.25645071]]),\n",
       " array([[0.46692244, 0.46692244, 0.46692244],\n",
       "        [0.57336718, 0.57336718, 0.57336718],\n",
       "        [0.45574459, 0.45574459, 0.45574459],\n",
       "        [0.43750763, 0.43750763, 0.43750763],\n",
       "        [0.51509687, 0.51509687, 0.51509687]]),\n",
       " array([[-0.41448969, -0.41448969, -0.41448969],\n",
       "        [-0.24834578, -0.24834578, -0.24834578],\n",
       "        [ 0.05871416,  0.05871416,  0.05871416],\n",
       "        [ 0.18506013,  0.18506013,  0.18506013],\n",
       "        [-0.25647662, -0.25647662, -0.25647662]]),\n",
       " array([[-0.41388529, -0.41388529, -0.41388529],\n",
       "        [-0.2483504 , -0.2483504 , -0.2483504 ],\n",
       "        [ 0.05869936,  0.05869936,  0.05869936],\n",
       "        [ 0.18425784,  0.18425784,  0.18425784],\n",
       "        [-0.25646405, -0.25646405, -0.25646405]]),\n",
       " array([[0.78532062, 0.78532062, 0.78532062]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
